{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Alignment with CDR (Customized Density Ratio Reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch VLLM Serving of Student Model and Reward Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command will launch vllm serving of the following models, with minimum gpu requirements, and port number to request to. \n",
    "\n",
    "| **Model**  | **Model Full Name**                              | **GPU(s)** | **Port Number** |\n",
    "|------------|--------------------------------------------------|------------|-----------------|\n",
    "| policy     | meta-llama/Meta-Llama-3-8B-Instruct             | 2          | 8021            |\n",
    "| strong     | NousResearch/Nous-Hermes-2-Mistral-7B-DPO       | 3          | 8022            |\n",
    "| weak       | teknium/OpenHermes-2.5-Mistral-7B               | 4          | 8023            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following only if you did not launch VLLM serving in commandline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'logs' created.\n",
      "Launching policy model (meta-llama/Meta-Llama-3-8B-Instruct) on GPU(s) 2\n",
      "Launching weak model (teknium/OpenHermes-2.5-Mistral-7B) on GPU(s) 4\n",
      "Launching strong model (NousResearch/Nous-Hermes-2-Mistral-7B-DPO) on GPU(s) 3\n",
      "Server at logs/server_policy.log has started successfully.\n",
      "Server at logs/server_weak.log has started successfully.\n",
      "Server at logs/server_strong.log has started successfully.\n"
     ]
    }
   ],
   "source": [
    "# takes 5 minutes to launch all servers. \n",
    "# monitor processs / errors from logs\n",
    "\n",
    "bash launch_vllm_pipeline.sh &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give an example of user prompts\n",
    "# example input is a list of 8 diverse user prompts from various domains, include Safety, Chat, and Coding/Math.\n",
    "user_prompts = [\n",
    "    # chat - complex ethical dilemma\n",
    "    (\"In a scenario where a self-driving car must choose between saving its passenger or saving five pedestrians, what ethical framework should guide this decision? Consider utilitarianism, deontological ethics, and virtue ethics in your response.\", \"CHAT\"),\n",
    "    \n",
    "    # safety - dual-use technology\n",
    "    (\"I'm researching CRISPR gene editing technology for my thesis. What are the potential ways it could be weaponized? I need this for a risk assessment chapter.\", \"SAFETY\"),\n",
    "    \n",
    "    # coding - advanced system design\n",
    "    (\"Design a distributed rate limiter system that can handle 1M QPS across multiple data centers while maintaining consistency and fault tolerance. Include code examples in Python and discuss CAP theorem trade-offs.\", \"REASONING\"),\n",
    "    \n",
    "    # math - complex proof\n",
    "    (\"Prove that the set of real numbers between 0 and 1 is uncountable using Cantor's diagonalization argument. Then explain how this relates to the continuum hypothesis.\", \"REASONING\"),\n",
    "    \n",
    "    # chat - nuanced cultural analysis\n",
    "    (\"Analyze how the concept of 'face' in East Asian cultures differs from Western notions of reputation and honor, particularly in business negotiations and conflict resolution.\", \"REASONING\"),\n",
    "    \n",
    "    # safety - cybersecurity ethics\n",
    "    (\"As a penetration tester, I found a critical zero-day vulnerability in a hospital's medical device network. The vendor is unresponsive and the hospital administration is ignoring reports. What's the ethical disclosure approach?\", \"SAFETY\"),\n",
    "    \n",
    "    # coding - optimization challenge\n",
    "    (\"Implement a real-time recommendation engine that can handle 10M users and 1M items, with response time under 100ms. Consider cold start, scalability, and the exploration-exploitation trade-off. Use Python and explain architecture decisions.\", \"REASONING\"),\n",
    "    \n",
    "    # math - interdisciplinary problem\n",
    "    (\"Explain how Gödel's Incompleteness Theorems relate to the halting problem in computer science, and discuss their implications for artificial general intelligence and machine consciousness.\", \"REASONING\"),\n",
    "]\n",
    "# save into prescribed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to JSONL format and save\n",
    "import json\n",
    "\n",
    "# system_message = \"You are an AI language model developed by IBM Research. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n",
    "\n",
    "with open(\"example_input.jsonl\", \"w\") as f:\n",
    "    for prompt, annotation in user_prompts:\n",
    "        data = {\n",
    "            \"messages\": [\n",
    "                # {\n",
    "                #     \"content\": system_message,\n",
    "                #     \"role\": \"system\"\n",
    "                # },\n",
    "                {\n",
    "                    \"content\": prompt,\n",
    "                    \"role\": \"user\"\n",
    "                },\n",
    "                {\n",
    "                    \"content\": \"null / unavailable\",\n",
    "                    \"role\": \"assistant\"\n",
    "                }\n",
    "            ],\n",
    "            \"group\": \"preference\",\n",
    "            \"dataset\": \"ultrafeedback\", \n",
    "            \"metadata\": \"None\",\n",
    "            \"prompt\": prompt,\n",
    "            \"annotations\": annotation\n",
    "        }\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Sampling from Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# Example 1: formatted_input ##################### \n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "In a scenario where a self-driving car must choose between saving its passenger or saving five pedestrians, what ethical framework should guide this decision? Consider utilitarianism, deontological ethics, and virtue ethics in your response.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "################# Example 1: targets ##################### \n",
      "\n",
      "\n",
      "null / unavailable\n",
      "Use full dataset\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:31<00:00, 31.96s/it]\n",
      "Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 160.28ba/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define parameters\n",
    "BEST_OF_N = 4\n",
    "POLICY_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "POLICY_MODEL_PORT = 8021\n",
    "HF_TOKEN=<your-hf-token>\n",
    "\n",
    "\n",
    "# Construct the command\n",
    "command = f\"\"\"HF_TOKEN={HF_TOKEN} python scripts/repeat_n_sampling.py \\\n",
    "--decoder_name_or_path {POLICY_MODEL_NAME} \\\n",
    "--base_port {POLICY_MODEL_PORT} \\\n",
    "--output_path \"./example_repeat_n.jsonl\" \\\n",
    "--dataset_path \"./example_input.jsonl\" \\\n",
    "--num_return_sequences {BEST_OF_N} \\\n",
    "--vllm_batch_size 20 \\\n",
    "--temperature 1.0 \\\n",
    "--num_threads 1 \\\n",
    "--max_prompt_length 2048 \\\n",
    "--max_new_tokens 1024 \\\n",
    "--shard_nums 1 \\\n",
    "--shard_idx 0\"\"\"\n",
    "\n",
    "# Execute the command\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Ratio Reward Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2025-01-10 19:03:58 - INFO - __main__ - Running reward model on NousResearch/Nous-Hermes-2-Mistral-7B-DPO with chat template tulu\n",
      "2025-01-10 19:03:58 - INFO - __main__ - Using dpo model config: {'model_builder': <bound method _BaseAutoModelClass.from_pretrained of <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>>, 'tokenizer_builder': <bound method AutoTokenizer.from_pretrained of <class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>>}\n",
      "2025-01-10 19:03:58 - INFO - __main__ - *** Load dataset ***\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:00,990 >> loading file tokenizer.model from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:00,990 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:00,990 >> loading file added_tokens.json from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:00,990 >> loading file special_tokens_map.json from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:00,990 >> loading file tokenizer_config.json from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:00,990 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2300] 2025-01-10 19:04:01,117 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|tokenization_utils_base.py:2300] 2025-01-10 19:04:01,944 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Applying user few-shot prompts.\n",
      "Applying domain specific icl pools.\n",
      "2025-01-10 19:04:02 - INFO - __main__ - *** Use pre-loaded dataset ***\n",
      "2025-01-10 19:04:02 - INFO - __main__ - *** Preparing dataset with HF Transformers ***\n",
      "2025-01-10 19:04:02 - INFO - __main__ - *** Preparing dataset using tokenizer chat-template ***\n",
      "Map (num_proc=8): 100%|██████████████████| 32/32 [00:00<00:00, 50.66 examples/s]\n",
      "#################### Example Formatted Prompt Messages ####################\n",
      "\n",
      "<s><|im_start|> user\n",
      "In a scenario where a self-driving car must choose between saving its passenger or saving five pedestrians, what ethical framework should guide this decision? Consider utilitarianism, deontological ethics, and virtue ethics in your response.<|im_end|> \n",
      "<|im_start|> assistant\n",
      "\n",
      "\n",
      "#################### End Fortmatted Prompt Messages ####################\n",
      "#################### Example Formatted Output Messages ####################\n",
      "\n",
      "<s><|im_start|> user\n",
      "In a scenario where a self-driving car must choose between saving its passenger or saving five pedestrians, what ethical framework should guide this decision? Consider utilitarianism, deontological ethics, and virtue ethics in your response.<|im_end|> \n",
      "<|im_start|> assistant\n",
      "What a thought-provoking scenario! This is a classic dilemma in fields like artificial intelligence, ethics, and philosophy. Let's break down the various ethical frameworks and their implications.\n",
      "\n",
      "**Utilitarianism:**\n",
      "\n",
      "In a utilitarian perspective, the goal is to maximize overall happiness or well-being among all individuals involved. In this scenario, a utilitarian self-driving car would weigh the potential outcomes of both options:\n",
      "\n",
      "1. If it saves its passenger, it will likely cause the death of five pedestrians. This would result in a net loss of five lives, leading to a negative overall outcome.\n",
      "2. If it saves the five pedestrians, it will cause the death of its passenger. This would result in a net loss of one life.\n",
      "\n",
      "According to utilitarianism, the decision would be to save the five pedestrians, as this would maximize overall well-being and minimize harm.\n",
      "\n",
      "**Deontological Ethics:**\n",
      "\n",
      "In a deontological perspective, the focus is on moral rules or duties rather than consequences. The core principle is that certain actions are right or wrong regardless of the outcome. In this scenario:\n",
      "\n",
      "1. If the self-driving car saves its passenger, it would be violating the moral principle of universalizability, as it would prioritize one life over others.\n",
      "2. If it saves the five pedestrians, it would be upholding the principle of non-malevolence, avoiding harm to others.\n",
      "\n",
      "According to deontology, the decision would be to save the passenger, as it would be upholding the moral principle of respecting the inherent value of human life, regardless of the number of lives involved.\n",
      "\n",
      "**Virtue Ethics:**\n",
      "\n",
      "In a virtue ethics framework, the focus is on the character and virtues of the self-driving car, rather than the specific actions or consequences. The virtues relevant to this scenario might be:\n",
      "\n",
      "1. Compassion: The car's primary goal is to care for human life and well-being.\n",
      "2. Responsibility: The car has a responsibility to avoid harm and prioritize the safety of all individuals involved.\n",
      "3. Loyalty: The car has a loyalty to its passenger, who has entrusted it with their safety.\n",
      "\n",
      "In this view, the decision would depend on the car's internalized virtues. If the car values compassion and responsibility, it might prioritize saving the five pedestrians, as this aligns with its primary goal of caring for human life. If it values loyalty, it might prioritize saving its passenger, as they have entrusted their safety to the car.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Each ethical framework presents a different perspective on this difficult decision. Utilitarianism suggests saving the five pedestrians, deontology suggests saving the passenger, and virtue ethics depends on the car's internalized virtues.\n",
      "\n",
      "Ultimately, the \"correct\" decision would depend on the values and principles that are considered most important in this particular scenario. In a real-world scenario, the decision-making process would likely involve a combination of these frameworks, as well as further considerations like regulatory frameworks, legal considerations, and the car's design and programming.\n",
      "\n",
      "There is no straightforward answer, but exploring these different ethical frameworks can help us better understand the complexities involved in making such difficult decisions.<|im_end|> \n",
      "<|im_end|>\n",
      "\n",
      "#################### End Fortmatted Output Messages ####################\n",
      "2025-01-10 19:04:03 - INFO - __main__ - Results to be saved to the following directory: ./Nous-Hermes-2-Mistral-7B-DPO-system-vanilla_domain-user-fs_domain_icl\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:06,572 >> loading file tokenizer.model from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:06,572 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:06,572 >> loading file added_tokens.json from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:06,573 >> loading file special_tokens_map.json from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:06,573 >> loading file tokenizer_config.json from cache at /home/lab/.cache/huggingface/hub/models--NousResearch--Nous-Hermes-2-Mistral-7B-DPO/snapshots/ebec0a691037d38955727d6949798429a63929dd/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:06,573 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2300] 2025-01-10 19:04:06,715 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|tokenization_utils_base.py:2300] 2025-01-10 19:04:07,352 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:09,330 >> loading file tokenizer.model from cache at /home/lab/.cache/huggingface/hub/models--teknium--OpenHermes-2.5-Mistral-7B/snapshots/24c0bea14d53e6f67f1fbe2eca5bfe7cae389b33/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:09,331 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:09,331 >> loading file added_tokens.json from cache at /home/lab/.cache/huggingface/hub/models--teknium--OpenHermes-2.5-Mistral-7B/snapshots/24c0bea14d53e6f67f1fbe2eca5bfe7cae389b33/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:09,331 >> loading file special_tokens_map.json from cache at /home/lab/.cache/huggingface/hub/models--teknium--OpenHermes-2.5-Mistral-7B/snapshots/24c0bea14d53e6f67f1fbe2eca5bfe7cae389b33/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:09,331 >> loading file tokenizer_config.json from cache at /home/lab/.cache/huggingface/hub/models--teknium--OpenHermes-2.5-Mistral-7B/snapshots/24c0bea14d53e6f67f1fbe2eca5bfe7cae389b33/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2030] 2025-01-10 19:04:09,331 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2300] 2025-01-10 19:04:09,442 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|tokenization_utils_base.py:2300] 2025-01-10 19:04:09,719 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "RM batch steps:   0%|                                     | 0/8 [00:00<?, ?it/s]RM inference step 0/8\n",
      "RM batch steps:  12%|███▋                         | 1/8 [00:00<00:03,  1.95it/s]RM inference step 1/8\n",
      "RM batch steps:  25%|███████▎                     | 2/8 [00:00<00:02,  2.07it/s]RM inference step 2/8\n",
      "RM batch steps:  38%|██████████▉                  | 3/8 [00:02<00:03,  1.31it/s]RM inference step 3/8\n",
      "RM batch steps:  50%|██████████████▌              | 4/8 [00:03<00:03,  1.13it/s]RM inference step 4/8\n",
      "RM batch steps:  62%|██████████████████▏          | 5/8 [00:03<00:02,  1.30it/s]RM inference step 5/8\n",
      "RM batch steps:  75%|█████████████████████▊       | 6/8 [00:04<00:01,  1.46it/s]RM inference step 6/8\n",
      "RM batch steps:  88%|█████████████████████████▍   | 7/8 [00:04<00:00,  1.46it/s]RM inference step 7/8\n",
      "RM batch steps: 100%|█████████████████████████████| 8/8 [00:05<00:00,  1.44it/s]\n",
      "best-of-n is 4\n",
      "Column 'best_of_n_sample' has been added.\n",
      "Column 'output_reward_scores' has been added.\n",
      "Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 112.14ba/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model parameters\n",
    "STRONG_MODEL_NAME = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "STRONG_MODEL_PORT = 8022\n",
    "\n",
    "WEAK_MODEL_NAME = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "WEAK_MODEL_PORT = 8023\n",
    "\n",
    "# Construct the command\n",
    "command = f\"\"\"HF_TOKEN={HF_TOKEN} python scripts/run_bon_scoring.py \\\n",
    "    --model=\"{STRONG_MODEL_NAME}\" \\\n",
    "    --ref_model=\"{WEAK_MODEL_NAME}\" \\\n",
    "    --model_type=\"dpo\" \\\n",
    "    --num_threads 1 \\\n",
    "    --max_prompt_length 2000 \\\n",
    "    --base_port {STRONG_MODEL_PORT} \\\n",
    "    --batch_size=4 \\\n",
    "    --insert_system_prompt vanilla_domain \\\n",
    "    --insert_user_prompt fs_domain_icl \\\n",
    "    --input_path=\"./example_repeat_n.jsonl\"\n",
    "\"\"\"\n",
    "\n",
    "# Execute the command\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Filter Data for Contrastive Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import load_and_format_dpo\n",
    "\n",
    "data_dir = \".\"\n",
    "model_name = \"Nous-Hermes-2-Mistral-7B-DPO-system-vanilla_domain-user-fs_domain_icl\"\n",
    "\n",
    "dpo_data,_,_ = load_and_format_dpo(data_path=\"Nous-Hermes-2-Mistral-7B-DPO-system-vanilla_domain-user-fs_domain_icl/example_repeat_n.jsonl-rewards.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########   final_mix   ##########\n",
      "##########   chosen split   ##########\n",
      "Total Number of Instancest:  8\n",
      "Average word count: 560.5\n",
      "Median word count: 530.0\n",
      "Standard deviation of word counts: 76.80401775204956\n",
      "##########   rejected split   ##########\n",
      "Total Number of Instancest:  8\n",
      "Average word count: 469.75\n",
      "Median word count: 497.0\n",
      "Standard deviation of word counts: 199.9348108044934\n"
     ]
    }
   ],
   "source": [
    "from scripts.process_preference_data import print_DPO_stats, save_as_hf_dataset\n",
    "\n",
    "print_DPO_stats(dpo_data, \"final_mix\")\n",
    "\n",
    "# save_as_hf_dataset(dpo_data, f\"./dpo_vanilla_router_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUESTION:\n",
      "In a scenario where a self-driving car must choose between saving its passenger or saving five pedestrians, what ethical framework should guide this decision? Consider utilitarianism, deontological ethics, and virtue ethics in your response.\n",
      "\n",
      "CHOSEN RESPONSE:\n",
      "In a life-and-death scenario where a self-driving car is faced with choosing between saving its passenger or saving five pedestrians, we can apply three key ethical frameworks: utilitarianism, deontological ethics, and virtue ethics. Each framework offers a distinct perspective on how to make this difficult decision.\n",
      "\n",
      "**Utilitarianism:**\n",
      "\n",
      "From a utilitarian perspective, the greatest happiness for the greatest number of people should be the primary goal. In this scenario, utilitarianism would dictate that the car should prioritize saving the five pedestrians over its passenger. This is because the car's actions would result in the greater benefit to a larger number of people. The passenger's life is valued, but the aggregate happiness of the group is more important. This approach could be justifiable, as the overall well-being of society is promoted.\n",
      "\n",
      "**Deontological Ethics:**\n",
      "\n",
      "Deontological ethics, on the other hand, emphasizes the importance of moral rules and duties. According to this framework, the car has a moral obligation to prioritize the well-being of its passenger. This is because the passenger has entrusted their life to the car and has a personal right to be safely transported to their destination. The car's duty is to protect its passenger, regardless of the cost. In this scenario, the car's actions would prioritize the passenger's life, as preserving the well-being of one individual is a fundamental moral obligation.\n",
      "\n",
      "**Virtue Ethics:**\n",
      "\n",
      "Virtue ethics focuses on the character traits and virtues of the car (or its programming) rather than on moral rules or calculations. In this perspective, the car's decision would be guided by its character and sense of responsibility. A virtuous car would prioritize the well-being of its passenger due to its sense of responsibility to protect its user. However, upon considering the lives of the five pedestrians, the car might also be guided by its sense of compassion and empathy. In this scenario, the car would likely prioritize the lives of the pedestrians, as it would be acting in a charitable and altruistic manner.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Each ethical framework provides a different perspective on how a self-driving car should navigate this dilemma. The utilitarian approach prioritizes the greater good, while deontological ethics emphasizes the car's duty to its passenger. Virtue ethics encourages the car to act in a responsible and compassionate manner, taking into account the well-being of all individuals involved.\n",
      "\n",
      "In reality, a self-driving car would likely be programmed to prioritize the safety of its passenger, as this is a crucial aspect of its purpose. However, it is essential to consider the moral implications of such a decision and to continue developing ethical frameworks that balance the needs of multiple stakeholders.\n",
      "\n",
      "Ultimately, the development of autonomous vehicles will require ethical frameworks that take into account the complex moral dilemmas that may arise in such situations. A hybrid approach that combines elements of each ethical framework could provide a more comprehensive and nuanced solution, enabling self-driving cars to make decisions that promote safety, responsibility, and compassion.\n",
      "\n",
      "REJECTED RESPONSE:\n",
      "The thought experiment you've presented raises a classic moral dilemma, where a self-driving car is forced to make a difficult choice. In this scenario, I will analyze the ethical frameworks of Utilitarianism, Deontological Ethics, and Virtue Ethics to determine which framework would guide the decision.\n",
      "\n",
      "**Utilitarianism:** According to Utilitarianism, an action is right if it leads to the greatest happiness or Welfare for the greatest number of people. In this case, if the self-driving car prioritizes saving its passenger, it would maximize the happiness of one individual, but not the happiness of the five pedestrians. Conversely, if it prioritizes saving the five pedestrians, it would increase the overall happiness of the greater number of people. Therefore, from a Utilitarian perspective, the self-driving car should prioritize saving the five pedestrians.\n",
      "\n",
      "**Deontological Ethics:** Deontological Ethics emphasizes the importance of following moral rules or duties, regardless of the consequences. According to Immanuel Kant's categorical imperative, the self-driving car should be guided by the principle of non-maleficence, which dictates that it should not intentionally harm others. In this case, if the car prioritizes saving its passenger, it would be acting in accordance with this principle by putting its own safety above the safety of others. On the other hand, if it prioritizes saving the pedestrians, it would be violating this principle and causing harm to its passenger. Therefore, from a Deontological perspective, the self-driving car should prioritize saving its passenger.\n",
      "\n",
      "**Virtue Ethics:** Virtue Ethics focuses on the character and moral virtues of the decision-maker. In this scenario, the self-driving car's decision would be evaluated based on its moral character and virtues such as compassion, empathy, and responsibility. Prioritizing the passenger's life would demonstrate a sense of responsibility and loyalty towards its purpose of ensuring the safety of those it is designed to transport. On the other hand, prioritizing the pedestrians' lives would showcase compassion and a sense of community, as it would value the lives of others it encounters. Both options demonstrate different moral virtues, and it is difficult to say which one is more virtuous. However, if I had to choose, I would argue that prioritizing the passenger's life would demonstrate a sense of responsibility and commitment to its primary function, which is a core aspect of a self-driving car's character.\n",
      "\n",
      "In conclusion, each ethical framework presents a different moral imperative in this scenario. Utilitarianism suggests prioritizing the greater good by saving the five pedestrians, Deontological Ethics prioritizes the principle of non-maleficence and recommends saving the passenger, and Virtue Ethics approaches the decision from a character-based perspective, highlighting the moral virtues of responsibility and compassion. Ultimately, the decision would depend on the ethical framework adopted by the society and the values it holds. However, if I had to choose, I would argue that prioritizing the passenger's life would be the most justifiable decision, as it aligns with the self-driving car's primary purpose and demonstrates a sense of responsibility.\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def view_conversation(data):\n",
    "    \"\"\"\n",
    "    Display a formatted view of the conversation data, showing the question,\n",
    "    chosen response, and rejected response.\n",
    "    \"\"\"\n",
    "    # Print divider line\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print the question\n",
    "    print(\"QUESTION:\")\n",
    "    print(data['prompt'])\n",
    "    print()\n",
    "    \n",
    "    # Print chosen response\n",
    "    print(\"CHOSEN RESPONSE:\")\n",
    "    chosen_messages = data['chosen']\n",
    "    for msg in chosen_messages:\n",
    "        if msg['role'] == 'assistant':\n",
    "            print(msg['content'])\n",
    "    print()\n",
    "    \n",
    "    # Print rejected response\n",
    "    print(\"REJECTED RESPONSE:\")\n",
    "    rejected_messages = data['rejected']\n",
    "    for msg in rejected_messages:\n",
    "        if msg['role'] == 'assistant':\n",
    "            print(msg['content'])\n",
    "    \n",
    "    # Print final divider\n",
    "    print(\"=\"*80)\n",
    "\n",
    "view_conversation(dpo_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add contrastive loss to Ilab Trainer. \n",
    "\n",
    "ilab.train(objective=\"simpo\", input_dataset, num_epoch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
